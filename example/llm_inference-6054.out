Running inference with fine-tuned model
Using device: cuda:0
Loading tokenizer...
Loading base model...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.25it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  3.24it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  2.99it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  4.00it/s]
Loading LoRA adapter from finetuned_llama...
Model successfully loaded to GPU
Processing questions from questions.txt...
Found 7 questions to process.
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:05<00:31,  5.21s/it] 29%|██▊       | 2/7 [00:31<01:27, 17.48s/it] 43%|████▎     | 3/7 [00:43<01:00, 15.16s/it] 57%|█████▋    | 4/7 [01:34<01:27, 29.27s/it] 71%|███████▏  | 5/7 [01:37<00:39, 19.88s/it] 86%|████████▌ | 6/7 [01:42<00:14, 14.81s/it]100%|██████████| 7/7 [04:49<00:00, 70.90s/it]100%|██████████| 7/7 [04:49<00:00, 41.31s/it]
Responses saved to answers_finetuned.txt
